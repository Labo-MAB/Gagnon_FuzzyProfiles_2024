{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMPORTING RELEVANT DATASETS AND FEATURES SELECTION**\n",
    "\n",
    "This section will import all the relevant dataset needed to run the analysis\n",
    "from Gagnon et al., 2024. Feature selection will be performed to keep only the\n",
    "relevant variables. \n",
    "\n",
    "#### **Setting up relevant paths.**\n",
    "\n",
    "In order for the following analyses to work, please update the following\n",
    "variables in the next cell.\n",
    "\n",
    "1. `repository_path`: should point to the location of the git repository.\n",
    "1. `abcd_base_path`: should point to the base folder of the abcd data release.\n",
    "1. `output_folder`: should point to a folder in which results will be outputted throughout the analyses. \n",
    "\n",
    "#### **Requirements.**\n",
    "\n",
    "To be able to run the following code, it is mandatory to install the NeuroStatX\n",
    "toolbox (https://github.com/gagnonanthony/NeuroStatX.git). If it isn't already\n",
    "install on your machine, please follow the instructions\n",
    "on the repository/documentation. All the analyses should be\n",
    "runnable on a entry-level computer (time to complete some steps might vary,\n",
    "long running steps are label by **This is a long running process. Go get a\n",
    "coffee !**). You may choose to skip some of these steps if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from neurostatx.io.utils import load_df_in_any_format\n",
    "from neurostatx.utils.preprocessing import merge_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up relevant paths.\n",
    "repository_path = \"/Users/anthonygagnon/code/Gagnon_LongitudinalProfiles/\" # CHANGE THIS\n",
    "abcd_base_path = \"/Volumes/T7/CCPM/ABCD/Release_5.1/abcd-data-release-5.1/\" # CHANGE THIS\n",
    "output_folder = \"/Volumes/T7/CCPM/RESULTS_JUNE_24/\" # CHANGE THIS\n",
    "\n",
    "# Setting up the paths for output subfolder.\n",
    "output_dir = f\"{output_folder}/LongitudinalProfiles/datagathering/\" # DO NOT CHANGE THIS\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fetching relevant data from the ABCD Study (Release 5.1)**\n",
    "\n",
    "Subsequent cells will load tables from the ABCD Release 5.1, filter to keep only baseline, 2-year, and 4-year follow-up data. Final tables will be outputted in the above specified `output_dir`.\n",
    "\n",
    "**Please note that this dataset is available through a data use certificate. For more informations, please see the [NIMH Data Archive website](https://nda.nih.gov/) or the [ABCD Study wiki](https://wiki.abcdstudy.org/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Starting with data data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonygagnon/envs/neurostatx-0.1.0/lib/python3.11/site-packages/neurostatx/io/utils.py:25: DtypeWarning: Columns (124,128,132,136,140,144,148,152,156,160,164,168,172,176,180,184,188,192,196,200) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/Users/anthonygagnon/envs/neurostatx-0.1.0/lib/python3.11/site-packages/neurostatx/io/utils.py:25: DtypeWarning: Columns (49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/Users/anthonygagnon/envs/neurostatx-0.1.0/lib/python3.11/site-packages/neurostatx/io/utils.py:25: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/Users/anthonygagnon/envs/neurostatx-0.1.0/lib/python3.11/site-packages/neurostatx/io/utils.py:25: DtypeWarning: Columns (274) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/Users/anthonygagnon/envs/neurostatx-0.1.0/lib/python3.11/site-packages/neurostatx/io/utils.py:25: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    }
   ],
   "source": [
    "# Load all necessary dataframes for neurocognition and behavior.\n",
    "cbcl = load_df_in_any_format(f'{abcd_base_path}/core/mental-health/mh_p_cbcl.csv')\n",
    "nihtb = load_df_in_any_format(f'{abcd_base_path}/core/neurocognition/nc_y_nihtb.csv')\n",
    "lmt = load_df_in_any_format(f'{abcd_base_path}/core/neurocognition/nc_y_lmt.csv')\n",
    "ravlt = load_df_in_any_format(f'{abcd_base_path}/core/neurocognition/nc_y_ravlt.csv')\n",
    "wisc = load_df_in_any_format(f'{abcd_base_path}/core/neurocognition/nc_y_wisc.csv')\n",
    "dice = load_df_in_any_format(f\"{abcd_base_path}/core/neurocognition/nc_y_gdt.csv\")\n",
    "bird = load_df_in_any_format(f\"{abcd_base_path}/core/neurocognition/nc_y_bird.csv\")\n",
    "\n",
    "# Load all necessary dataframes for covariates/demographics.\n",
    "lt = load_df_in_any_format(f'{abcd_base_path}/core/abcd-general/abcd_y_lt.csv')\n",
    "hand = load_df_in_any_format(f'{abcd_base_path}/core/neurocognition/nc_y_ehis.csv')\n",
    "vision = load_df_in_any_format(f'{abcd_base_path}/core/neurocognition/nc_y_svs.csv')\n",
    "demo = load_df_in_any_format(f\"{abcd_base_path}/core/abcd-general/abcd_p_demo.csv\")\n",
    "agemonth = load_df_in_any_format(f'{abcd_base_path}/core/abcd-general/abcd_y_lt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/23/p0fh1nrd473dtfz3cx_4g0j00000gn/T/ipykernel_78865/2761186772.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  demo_data.loc[:, \"high_edu\"] = demo_data[[\"Parent_ed1\", \"Parent_ed2\"]].values.max(1)\n",
      "/var/folders/23/p0fh1nrd473dtfz3cx_4g0j00000gn/T/ipykernel_78865/2761186772.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  demo_data.loc[:, 'ParentalEducation'] = demo_data['high_edu'].apply(create_edu_groups)\n",
      "/var/folders/23/p0fh1nrd473dtfz3cx_4g0j00000gn/T/ipykernel_78865/2761186772.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  demo_data.loc[:, 'IncomeGroups'] = demo_data['Income'].apply(create_income_groups)\n"
     ]
    }
   ],
   "source": [
    "# Building demographics and covariates data.\n",
    "demo_baseline = demo[demo.eventname == \"baseline_year_1_arm_1\"]\n",
    "agemonth_baseline = agemonth[agemonth.eventname == \"baseline_year_1_arm_1\"]\n",
    "lt_baseline = lt[lt.eventname == \"baseline_year_1_arm_1\"]\n",
    "hand_baseline = hand[hand.eventname == \"baseline_year_1_arm_1\"]\n",
    "vision_baseline = vision[vision.eventname == \"baseline_year_1_arm_1\"]\n",
    "\n",
    "# Extracting sites data.\n",
    "site_vars = [\"src_subject_id\", \"site_id_l\"]\n",
    "site = lt_baseline[site_vars]\n",
    "site.columns = [\"subjectkey\", \"Site\"]\n",
    "\n",
    "# Extracting demographics data.\n",
    "demo_vars = [\n",
    "    \"src_subject_id\",\n",
    "    \"demo_sex_v2\",\n",
    "    \"race_ethnicity\",\n",
    "    \"demo_prnt_ed_v2\",\n",
    "    \"demo_prtnr_ed_v2\",\n",
    "    \"demo_comb_income_v2\"\n",
    "]\n",
    "demo_data = demo_baseline[demo_vars]\n",
    "demo_data.columns = [\n",
    "    \"subjectkey\",\n",
    "    \"Sex\",\n",
    "    \"Ethnicity\",\n",
    "    \"Parent_ed1\",\n",
    "    \"Parent_ed2\",\n",
    "    \"Income\"\n",
    "]\n",
    "\n",
    "# Extracting age data.\n",
    "age_vars = [\"src_subject_id\", \"interview_age\"]\n",
    "age_data = agemonth_baseline[age_vars]\n",
    "age_data.columns = [\"subjectkey\", \"AgeMonths\"]\n",
    "\n",
    "# Extracting handedness data.\n",
    "hand_vars = [\"src_subject_id\", \"ehi_y_ss_scoreb\"]\n",
    "hand_data = hand_baseline[hand_vars]\n",
    "hand_data.columns = [\"subjectkey\", \"Handedness\"]\n",
    "\n",
    "# Invert the handedness score. 1 = left, 2 = right, 3 = ambidextrous.\n",
    "def invert_handedness(x):\n",
    "    if x == 1:\n",
    "        return 2\n",
    "    elif x == 2:\n",
    "        return 1\n",
    "    elif x == 3:\n",
    "        return 3\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "hand_data.loc[:, \"Handedness\"] = hand_data[\"Handedness\"].apply(invert_handedness)\n",
    "\n",
    "# Extracting vision data.\n",
    "vision_vars = [\"src_subject_id\", \"snellen_va_y\"]\n",
    "vision_data = vision_baseline[vision_vars]\n",
    "vision_data.columns = [\"subjectkey\", \"Vision\"]\n",
    "\n",
    "# Extract parental highest education level. Taking the highest among the two parents.\n",
    "demo_data.loc[:, \"Parent_ed1\"] = demo_data[\"Parent_ed1\"].replace([777, 999, np.nan], 0)\n",
    "demo_data.loc[:, \"Parent_ed2\"] = demo_data[\"Parent_ed2\"].replace([777, 999, np.nan], 0)\n",
    "demo_data.loc[:, \"high_edu\"] = demo_data[[\"Parent_ed1\", \"Parent_ed2\"]].values.max(1)\n",
    "\n",
    "# Group levels together (<13 = 1, no high school, 13-14 = 2, high school, ged or equivalent,\n",
    "# 15-17 = 3, some college, 18 = 4, bachelor, >19 = 5, postgraduate)\n",
    "def create_edu_groups(x):\n",
    "    if x < 13:\n",
    "        return 1\n",
    "    elif x in [13, 14]:\n",
    "        return 2\n",
    "    elif x in [15, 16, 17]:\n",
    "        return 3\n",
    "    elif x == 18:\n",
    "        return 4\n",
    "    elif x in [19, 20, 21]:\n",
    "        return 5\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "demo_data.loc[:, 'ParentalEducation'] = demo_data['high_edu'].apply(create_edu_groups)\n",
    "\n",
    "# Extracting the income groups.\n",
    "# Group levels of income together ( <6 = 1, < 50 000, 6-8 = 2, 50-100 000, >9 = 3, >100 000).\n",
    "def create_income_groups(x):\n",
    "    if x < 6:\n",
    "        return 1\n",
    "    elif x in [6, 7, 8]:\n",
    "        return 2\n",
    "    elif x in [9, 10]:\n",
    "        return 3\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "demo_data.loc[:, 'IncomeGroups'] = demo_data['Income'].apply(create_income_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a baseline dataframe first.\n",
    "cbcl_baseline = cbcl[cbcl.eventname == \"baseline_year_1_arm_1\"]\n",
    "nihtb_baseline = nihtb[nihtb.eventname == \"baseline_year_1_arm_1\"]\n",
    "lmt_baseline = lmt[lmt.eventname == \"baseline_year_1_arm_1\"]\n",
    "ravlt_baseline = ravlt[ravlt.eventname == \"baseline_year_1_arm_1\"]\n",
    "wisc_baseline = wisc[wisc.eventname == \"baseline_year_1_arm_1\"]\n",
    "\n",
    "# CBCL syndrome scores.\n",
    "cbcl_baseline = cbcl_baseline[[\"src_subject_id\", \"cbcl_scr_syn_internal_r\", \"cbcl_scr_syn_external_r\", \"cbcl_scr_07_stress_r\"]]\n",
    "cbcl_baseline.columns = [\"subjectkey\", \"Internalizing\", \"Externalizing\", \"Stress\"]\n",
    "\n",
    "# Scale the CBCL scores.\n",
    "cbcl_baseline.loc[:, \"Internalizing\"] = StandardScaler().fit_transform(cbcl_baseline[[\"Internalizing\"]])\n",
    "cbcl_baseline.loc[:, \"Externalizing\"] = StandardScaler().fit_transform(cbcl_baseline[[\"Externalizing\"]])\n",
    "cbcl_baseline.loc[:, \"Stress\"] = StandardScaler().fit_transform(cbcl_baseline[[\"Stress\"]])\n",
    "\n",
    "# NIHTB scores.\n",
    "nihtb_vars = [\n",
    "    \"src_subject_id\",\n",
    "    \"nihtbx_picvocab_uncorrected\",\n",
    "    \"nihtbx_flanker_uncorrected\",\n",
    "    \"nihtbx_list_uncorrected\",\n",
    "    \"nihtbx_cardsort_uncorrected\",\n",
    "    \"nihtbx_pattern_uncorrected\",\n",
    "    \"nihtbx_picture_uncorrected\",\n",
    "    \"nihtbx_reading_uncorrected\",\n",
    "]\n",
    "nihtb_baseline = nihtb_baseline[nihtb_vars]\n",
    "nihtb_baseline.columns = [\n",
    "    \"subjectkey\",\n",
    "    \"PictureVocab\",\n",
    "    \"Flanker\",\n",
    "    \"ListSorting\",\n",
    "    \"CardSort\",\n",
    "    \"PatternComparison\",\n",
    "    \"PictureSequence\",\n",
    "    \"OralReading\"\n",
    "]\n",
    "\n",
    "# Scale the NIHTB scores.\n",
    "nihtb_baseline.loc[:, \"PictureVocab\"] = StandardScaler().fit_transform(nihtb_baseline[[\"PictureVocab\"]])\n",
    "nihtb_baseline.loc[:, \"Flanker\"] = StandardScaler().fit_transform(nihtb_baseline[[\"Flanker\"]])\n",
    "nihtb_baseline.loc[:, \"ListSorting\"] = StandardScaler().fit_transform(nihtb_baseline[[\"ListSorting\"]])\n",
    "nihtb_baseline.loc[:, \"CardSort\"] = StandardScaler().fit_transform(nihtb_baseline[[\"CardSort\"]])\n",
    "nihtb_baseline.loc[:, \"PatternComparison\"] = StandardScaler().fit_transform(nihtb_baseline[[\"PatternComparison\"]])\n",
    "nihtb_baseline.loc[:, \"PictureSequence\"] = StandardScaler().fit_transform(nihtb_baseline[[\"PictureSequence\"]])\n",
    "nihtb_baseline.loc[:, \"OralReading\"] = StandardScaler().fit_transform(nihtb_baseline[[\"OralReading\"]])\n",
    "\n",
    "# Little Man's Task scores.\n",
    "lmt_baseline = lmt_baseline[[\"src_subject_id\", \"lmt_scr_perc_correct\"]]\n",
    "lmt_baseline.columns = [\"subjectkey\", \"LMT\"]\n",
    "\n",
    "# Scale the LMT scores.\n",
    "lmt_baseline.loc[:, \"LMT\"] = StandardScaler().fit_transform(lmt_baseline[[\"LMT\"]])\n",
    "\n",
    "# Rey Auditory Verbal Learning Test scores.\n",
    "ravlt_baseline = ravlt_baseline[[\"src_subject_id\", \"pea_ravlt_ld_trial_vii_tc\"]]\n",
    "ravlt_baseline.columns = [\"subjectkey\", \"RAVLT\"]\n",
    "\n",
    "# Scale the RAVLT scores.\n",
    "ravlt_baseline.loc[:, \"RAVLT\"] = StandardScaler().fit_transform(ravlt_baseline[[\"RAVLT\"]])\n",
    "\n",
    "# Wechsler Intelligence Scale for Children scores.\n",
    "wisc_baseline = wisc_baseline[[\"src_subject_id\", \"pea_wiscv_tss\"]]\n",
    "wisc_baseline.columns = [\"subjectkey\", \"WISCMatrix\"]\n",
    "\n",
    "# Scale the WISC scores.\n",
    "wisc_baseline.loc[:, \"WISCMatrix\"] = StandardScaler().fit_transform(wisc_baseline[[\"WISCMatrix\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Computing the diagnosis variables using the KSADS.\n",
    "!python {repository_path}/scripts/generate_dx_ABCD.py --in-root-folder {abcd_base_path} \\\n",
    "    --output {output_dir}/abcd_dx_labels_baseline.xlsx --eventname \"baseline_year_1_arm_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing back the diagnosis labels.\n",
    "dx_labels = load_df_in_any_format(f\"{output_dir}/abcd_dx_labels_baseline.xlsx\")\n",
    "\n",
    "# Add global psychopathology score (1 = at least one diagnosis, 0 = no diagnosis).\n",
    "dx_labels.loc[:, \"PSYPATHO\"] = dx_labels.iloc[:, 1:].sum(axis=1).apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects retained for the analysis: 10843\n"
     ]
    }
   ],
   "source": [
    "# First, merging all the cognitive and behavioral dataframes. Then, merging the demographics and covariates dataframes.\n",
    "merged_data = merge_dataframes({\"agemonth\": age_data, \"dx\": dx_labels, \"cbcl\": cbcl_baseline,\n",
    "                                \"nihtb\": nihtb_baseline, \"lmt\": lmt_baseline,\n",
    "                                \"ravlt\": ravlt_baseline, \"wisc\": wisc_baseline},\n",
    "                                index=\"subjectkey\")\n",
    "merged_data.dropna(inplace=True, axis=0)\n",
    "merged_data.reset_index(drop=False, inplace=True)\n",
    "print(\"Number of subjects retained for the analysis: {}\".format(merged_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline data gathering completed.\n"
     ]
    }
   ],
   "source": [
    "# Contatenating all dataframes.\n",
    "abcd_data_baseline = merge_dataframes({\"site\": site, \"demo\": demo_data,\n",
    "                                       \"hand\": hand_data, \"vision\": vision_data,\n",
    "                                       \"merged\": merged_data},\n",
    "                                      index=\"subjectkey\")\n",
    "\n",
    "# Dropping the rows with NA in the last 6 columns which corresponds to the cognitive and behavioral data.\n",
    "abcd_data_baseline.dropna(subset=abcd_data_baseline.columns[-6:], inplace=True, axis=0, how=\"all\")\n",
    "\n",
    "# Assert that the number of subjects is the same as before.\n",
    "assert abcd_data_baseline.shape[0] == merged_data.shape[0], \"Number of subjects do not match.\"\n",
    "\n",
    "# Save the final dataframe.\n",
    "abcd_data_baseline.to_excel(f\"{output_dir}/abcd_data_baseline.xlsx\", index=True, header=True)\n",
    "\n",
    "print(\"Baseline data gathering completed.\")\n",
    "\n",
    "# This next line is commented out since data is protected by a data use agreement.\n",
    "# abcd_data_baseline.head() # Please inspect head of the dataframe to validate correct merging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2-year follow-up data gathering.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Starting by computing the diagnosis labels.\n",
    "!python {repository_path}/scripts/generate_dx_ABCD.py --in-root-folder {abcd_base_path} \\\n",
    "    --output {output_dir}/abcd_dx_labels_2y.xlsx --eventname \"2_year_follow_up_y_arm_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing back the diagnosis labels.\n",
    "dx_labels = load_df_in_any_format(f\"{output_dir}/abcd_dx_labels_2y.xlsx\")\n",
    "\n",
    "# Add global psychopathology score (1 = at least one diagnosis, 0 = no diagnosis).\n",
    "dx_labels.loc[:, \"PSYPATHO\"] = dx_labels.iloc[:, 1:].sum(axis=1).apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Fetch age at 2 year follow-up.\n",
    "age_2y = agemonth[agemonth.eventname == \"2_year_follow_up_y_arm_1\"]\n",
    "age_vars = [\"src_subject_id\", \"interview_age\"]\n",
    "age_2y = age_2y[age_vars]\n",
    "age_2y.columns = [\"subjectkey\", \"AgeMonths\"]\n",
    "\n",
    "# Vision data at 2 year follow-up.\n",
    "vision_2y = vision[vision.eventname == \"2_year_follow_up_y_arm_1\"]\n",
    "vision_vars = [\"src_subject_id\", \"snellen_va_y\"]\n",
    "vision_2y = vision_2y[vision_vars]\n",
    "vision_2y.columns = [\"subjectkey\", \"Vision\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a 2y follow-up dataset.\n",
    "cbcl_2y = cbcl[cbcl.eventname == \"2_year_follow_up_y_arm_1\"]\n",
    "nihtb_2y = nihtb[nihtb.eventname == \"2_year_follow_up_y_arm_1\"]\n",
    "lmt_2y = lmt[lmt.eventname == \"2_year_follow_up_y_arm_1\"]\n",
    "ravlt_2y = ravlt[ravlt.eventname == \"2_year_follow_up_y_arm_1\"]\n",
    "dice_2y = dice[dice.eventname == \"2_year_follow_up_y_arm_1\"]\n",
    "\n",
    "# CBCL syndrome scores.\n",
    "cbcl_2y = cbcl_2y[[\"src_subject_id\", \"cbcl_scr_syn_internal_r\", \"cbcl_scr_syn_external_r\", \"cbcl_scr_07_stress_r\"]]\n",
    "cbcl_2y.columns = [\"subjectkey\", \"Internalizing\", \"Externalizing\", \"Stress\"]\n",
    "\n",
    "# Scale the CBCL scores.\n",
    "cbcl_2y.loc[:, \"Internalizing\"] = StandardScaler().fit_transform(cbcl_2y[[\"Internalizing\"]])\n",
    "cbcl_2y.loc[:, \"Externalizing\"] = StandardScaler().fit_transform(cbcl_2y[[\"Externalizing\"]])\n",
    "cbcl_2y.loc[:, \"Stress\"] = StandardScaler().fit_transform(cbcl_2y[[\"Stress\"]])\n",
    "\n",
    "# NIHTB scores.\n",
    "nihtb_vars = [\n",
    "    \"src_subject_id\",\n",
    "    \"nihtbx_flanker_uncorrected\",\n",
    "    \"nihtbx_picvocab_uncorrected\",\n",
    "    \"nihtbx_pattern_uncorrected\",\n",
    "    \"nihtbx_picture_uncorrected\",\n",
    "    \"nihtbx_reading_uncorrected\",\n",
    "]\n",
    "nihtb_2y = nihtb_2y[nihtb_vars]\n",
    "nihtb_2y.columns = [\n",
    "    \"subjectkey\",\n",
    "    \"Flanker\",\n",
    "    \"PictureVocab\",\n",
    "    \"PatternComparison\",\n",
    "    \"PictureSequence\",\n",
    "    \"OralReading\"\n",
    "]\n",
    "\n",
    "# Scale the NIHTB scores.\n",
    "nihtb_2y.loc[:, \"Flanker\"] = StandardScaler().fit_transform(nihtb_2y[[\"Flanker\"]])\n",
    "nihtb_2y.loc[:, \"PictureVocab\"] = StandardScaler().fit_transform(nihtb_2y[[\"PictureVocab\"]])\n",
    "nihtb_2y.loc[:, \"PatternComparison\"] = StandardScaler().fit_transform(nihtb_2y[[\"PatternComparison\"]])\n",
    "nihtb_2y.loc[:, \"PictureSequence\"] = StandardScaler().fit_transform(nihtb_2y[[\"PictureSequence\"]])\n",
    "nihtb_2y.loc[:, \"OralReading\"] = StandardScaler().fit_transform(nihtb_2y[[\"OralReading\"]])\n",
    "\n",
    "# Little Man's Task scores.\n",
    "lmt_2y = lmt_2y[[\"src_subject_id\", \"lmt_scr_perc_correct\"]]\n",
    "lmt_2y.columns = [\"subjectkey\", \"LMT\"]\n",
    "\n",
    "# Some scores are in % rather than in decimal. We need to convert them to decimal.\n",
    "# Apply only if the scores are above 1.\n",
    "def convert_lmt_scores(x):\n",
    "    if x > 1:\n",
    "        return x / 100\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "lmt_2y.loc[:, \"LMT\"] = lmt_2y[\"LMT\"].apply(convert_lmt_scores)\n",
    "\n",
    "# Scale the LMT scores.\n",
    "lmt_2y.loc[:, \"LMT\"] = StandardScaler().fit_transform(lmt_2y[[\"LMT\"]])\n",
    "\n",
    "# Rey Auditory Verbal Learning Test scores.\n",
    "ravlt_2y = ravlt_2y[[\"src_subject_id\", \"pea_ravlt_ld_trial_vii_tc\"]]\n",
    "ravlt_2y.columns = [\"subjectkey\", \"RAVLT\"]\n",
    "\n",
    "# Scale the RAVLT scores.\n",
    "ravlt_2y.loc[:, \"RAVLT\"] = StandardScaler().fit_transform(ravlt_2y[[\"RAVLT\"]])\n",
    "\n",
    "# Game of Dice scores.\n",
    "dice_2y = dice_2y[[\"src_subject_id\", \"gdt_scr_expressions_net_score\"]]\n",
    "dice_2y.columns = [\"subjectkey\", \"DICE\"]\n",
    "\n",
    "# Scale the DICE scores.\n",
    "dice_2y.loc[:, \"DICE\"] = StandardScaler().fit_transform(dice_2y[[\"DICE\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects retained for the analysis: 7369\n"
     ]
    }
   ],
   "source": [
    "# Merge all the cognitive and behavioral dataframes.\n",
    "merged_data = merge_dataframes({\"agemonth\": age_2y, \"dx\": dx_labels, \"cbcl\": cbcl_2y,\n",
    "                                \"nihtb\": nihtb_2y, \"lmt\": lmt_2y,\n",
    "                                \"ravlt\": ravlt_2y, \"dice\": dice_2y},\n",
    "                                index=\"subjectkey\")\n",
    "merged_data.dropna(inplace=True, axis=0)\n",
    "merged_data.reset_index(drop=False, inplace=True)\n",
    "print(\"Number of subjects retained for the analysis: {}\".format(merged_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 year follow-up data gathering completed.\n"
     ]
    }
   ],
   "source": [
    "# Merge with demographics.\n",
    "abcd_data_2y = merge_dataframes({\"site\": site, \"demo\": demo_data,\n",
    "                                 \"hand\": hand_data, \"vision\": vision_2y,\n",
    "                                 \"merged\": merged_data},\n",
    "                                index=\"subjectkey\")\n",
    "\n",
    "# Dropping the rows with NA in the last 6 columns which corresponds to the cognitive and behavioral data.\n",
    "abcd_data_2y.dropna(subset=abcd_data_2y.columns[-6:], inplace=True, axis=0, how=\"all\")\n",
    "\n",
    "# Assert that the number of subjects is the same as before.\n",
    "assert abcd_data_2y.shape[0] == merged_data.shape[0], \"Number of subjects do not match.\"\n",
    "\n",
    "# Save the final dataframe.\n",
    "abcd_data_2y.to_excel(f\"{output_dir}/abcd_data_2y.xlsx\", index=True, header=True)\n",
    "\n",
    "print(\"2 year follow-up data gathering completed.\")\n",
    "\n",
    "# This next line is commented out since data is protected by a data use agreement.\n",
    "# abcd_data_2y.head() # Please inspect head of the dataframe to validate correct merging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4-year follow-up data gathering.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Starting by computing the diagnosis labels. (not available as of 30/07/2024)\n",
    "# !python {repository_path}/scripts/generate_dx_ABCD.py --in-root-folder {abcd_base_path} \\\n",
    "#     --output {output_dir}/abcd_dx_labels_4y.xlsx --eventname \"4_year_follow_up_y_arm_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing back the diagnosis labels.\n",
    "# dx_labels = load_df_in_any_format(f\"{output_dir}/abcd_dx_labels_4y.xlsx\")\n",
    "\n",
    "# Add global psychopathology score (1 = at least one diagnosis, 0 = no diagnosis).\n",
    "# dx_labels.loc[:, \"PSYPATHO\"] = dx_labels.iloc[:, 1:].sum(axis=1).apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Fetch age at 4 year follow-up.\n",
    "age_4y = agemonth[agemonth.eventname == \"4_year_follow_up_y_arm_1\"]\n",
    "age_vars = [\"src_subject_id\", \"interview_age\"]\n",
    "age_4y = age_4y[age_vars]\n",
    "age_4y.columns = [\"subjectkey\", \"AgeMonths\"]\n",
    "\n",
    "# Vision data at 4 year follow-up.\n",
    "vision_4y = vision[vision.eventname == \"4_year_follow_up_y_arm_1\"]\n",
    "vision_vars = [\"src_subject_id\", \"snellen_va_y\"]\n",
    "vision_4y = vision_4y[vision_vars]\n",
    "vision_4y.columns = [\"subjectkey\", \"Vision\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a 4y follow-up dataset.\n",
    "cbcl_4y = cbcl[cbcl.eventname == \"4_year_follow_up_y_arm_1\"]\n",
    "nihtb_4y = nihtb[nihtb.eventname == \"4_year_follow_up_y_arm_1\"]\n",
    "lmt_4y = lmt[lmt.eventname == \"4_year_follow_up_y_arm_1\"]\n",
    "dice_4y = dice[dice.eventname == \"4_year_follow_up_y_arm_1\"]\n",
    "bird_4y = bird[bird.eventname == \"4_year_follow_up_y_arm_1\"]\n",
    "\n",
    "# CBCL syndrome scores.\n",
    "cbcl_4y = cbcl_4y[[\"src_subject_id\", \"cbcl_scr_syn_internal_r\", \"cbcl_scr_syn_external_r\", \"cbcl_scr_07_stress_r\"]]\n",
    "cbcl_4y.columns = [\"subjectkey\", \"Internalizing\", \"Externalizing\", \"Stress\"]\n",
    "\n",
    "# Scale the CBCL scores.\n",
    "cbcl_4y.loc[:, \"Internalizing\"] = StandardScaler().fit_transform(cbcl_4y[[\"Internalizing\"]])\n",
    "cbcl_4y.loc[:, \"Externalizing\"] = StandardScaler().fit_transform(cbcl_4y[[\"Externalizing\"]])\n",
    "cbcl_4y.loc[:, \"Stress\"] = StandardScaler().fit_transform(cbcl_4y[[\"Stress\"]])\n",
    "\n",
    "# NIHTB scores.\n",
    "nihtb_vars = [\n",
    "    \"src_subject_id\",\n",
    "    \"nihtbx_flanker_uncorrected\",\n",
    "    \"nihtbx_list_uncorrected\",\n",
    "    \"nihtbx_picvocab_uncorrected\",\n",
    "    \"nihtbx_pattern_uncorrected\",\n",
    "    \"nihtbx_picture_uncorrected\",\n",
    "    \"nihtbx_reading_uncorrected\",\n",
    "]\n",
    "nihtb_4y = nihtb_4y[nihtb_vars]\n",
    "nihtb_4y.columns = [\n",
    "    \"subjectkey\",\n",
    "    \"Flanker\",\n",
    "    \"ListSorting\",\n",
    "    \"PictureVocab\",\n",
    "    \"PatternComparison\",\n",
    "    \"PictureSequence\",\n",
    "    \"OralReading\"\n",
    "]\n",
    "\n",
    "# Scale the NIHTB scores.\n",
    "nihtb_4y.loc[:, \"Flanker\"] = StandardScaler().fit_transform(nihtb_4y[[\"Flanker\"]])\n",
    "nihtb_4y.loc[:, \"ListSorting\"] = StandardScaler().fit_transform(nihtb_4y[[\"ListSorting\"]])\n",
    "nihtb_4y.loc[:, \"PictureVocab\"] = StandardScaler().fit_transform(nihtb_4y[[\"PictureVocab\"]])\n",
    "nihtb_4y.loc[:, \"PatternComparison\"] = StandardScaler().fit_transform(nihtb_4y[[\"PatternComparison\"]])\n",
    "nihtb_4y.loc[:, \"PictureSequence\"] = StandardScaler().fit_transform(nihtb_4y[[\"PictureSequence\"]])\n",
    "nihtb_4y.loc[:, \"OralReading\"] = StandardScaler().fit_transform(nihtb_4y[[\"OralReading\"]])\n",
    "\n",
    "# Little Man's Task scores.\n",
    "lmt_4y = lmt_4y[[\"src_subject_id\", \"lmt_scr_perc_correct\"]]\n",
    "lmt_4y.columns = [\"subjectkey\", \"LMT\"]\n",
    "\n",
    "# Scale the LMT scores.\n",
    "lmt_4y.loc[:, \"LMT\"] = StandardScaler().fit_transform(lmt_4y[[\"LMT\"]])\n",
    "\n",
    "# Game of Dice scores.\n",
    "dice_4y = dice_4y[[\"src_subject_id\", \"gdt_scr_values_risky\"]]\n",
    "dice_4y.columns = [\"subjectkey\", \"DICE\"]\n",
    "\n",
    "# Scale the DICE scores.\n",
    "dice_4y.loc[:, \"DICE\"] = StandardScaler().fit_transform(dice_4y[[\"DICE\"]])\n",
    "\n",
    "# Bird scores.\n",
    "bird_4y = bird_4y[[\"src_subject_id\", \"bird_scr_score\"]]\n",
    "bird_4y.columns = [\"subjectkey\", \"BIRD\"]\n",
    "\n",
    "# Scale the BIRD scores.\n",
    "bird_4y.loc[:, \"BIRD\"] = StandardScaler().fit_transform(bird_4y[[\"BIRD\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects retained for the analysis: 2846\n"
     ]
    }
   ],
   "source": [
    "# Merge all the cognitive and behavioral dataframes.\n",
    "merged_data = merge_dataframes({\"agemonth\": age_4y, \"cbcl\": cbcl_4y,\n",
    "                                \"nihtb\": nihtb_4y, \"lmt\": lmt_4y,\n",
    "                                \"dice\": dice_4y, \"bird\": bird_4y},\n",
    "                                index=\"subjectkey\")\n",
    "merged_data.dropna(inplace=True, axis=0)\n",
    "merged_data.reset_index(drop=False, inplace=True)\n",
    "print(\"Number of subjects retained for the analysis: {}\".format(merged_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 year follow-up data gathering completed.\n"
     ]
    }
   ],
   "source": [
    "# Merge with demographics.\n",
    "abcd_data_4y = merge_dataframes({\"site\": site, \"demo\": demo_data,\n",
    "                                 \"hand\": hand_data, \"vision\": vision_4y,\n",
    "                                 \"merged\": merged_data},\n",
    "                                index=\"subjectkey\")\n",
    "\n",
    "# Dropping the rows with NA in the last 6 columns which corresponds to the cognitive and behavioral data.\n",
    "abcd_data_4y.dropna(subset=abcd_data_4y.columns[-6:], inplace=True, axis=0, how=\"all\")\n",
    "\n",
    "# Assert that the number of subjects is the same as before.\n",
    "assert abcd_data_4y.shape[0] == merged_data.shape[0], \"Number of subjects do not match.\"\n",
    "\n",
    "# Save the final dataframe.\n",
    "abcd_data_4y.to_excel(f\"{output_dir}/abcd_data_4y.xlsx\", index=True, header=True)\n",
    "\n",
    "print(\"4 year follow-up data gathering completed.\")\n",
    "\n",
    "# This next line is commented out since data is protected by a data use agreement.\n",
    "# abcd_data_4y.head() # Please inspect head of the dataframe to validate correct merging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurostatx-0.1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
